{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b838cb5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "experiments:\n",
    "implement 2-inputs float 32 logsumexp and our int32 logsumexp both in pytorch step by step.\n",
    "record the each step time and the total time.\n",
    "run this for 10 times.\n",
    "\n",
    "cpu: AMD EPYC 7413 24-Core Processor\n",
    "gpu: a100-80Gb\n",
    "\n",
    "CPU: Float32 logsumexp timings in 10 times run experiments:\n",
    "        max       min       sub       exp       sum       log       add     total\n",
    "0  0.272384  0.037888  0.093184  2.706432  0.198656  0.043008  0.013312  3.364864\n",
    "1  0.014336  0.012288  0.013312  0.013312  0.026624  0.013312  0.012288  0.105472\n",
    "2  0.013312  0.011264  0.012288  0.013312  0.019456  0.013312  0.010240  0.093184\n",
    "3  0.013312  0.011264  0.012288  0.012288  0.019456  0.013312  0.011264  0.093184\n",
    "4  0.012288  0.011264  0.011264  0.013312  0.020480  0.012288  0.011264  0.092160\n",
    "5  0.012288  0.012288  0.012288  0.012288  0.021504  0.012288  0.010240  0.093184\n",
    "6  0.012288  0.011264  0.011264  0.012288  0.018432  0.012288  0.010240  0.088064\n",
    "7  0.013312  0.011264  0.011264  0.013312  0.020480  0.013312  0.010240  0.093184\n",
    "8  0.012288  0.012288  0.011264  0.014336  0.020480  0.016384  0.012288  0.099328\n",
    "9  0.012288  0.011264  0.011264  0.013312  0.019456  0.013312  0.011264  0.092160\n",
    "CPU: our Int32 logsumexp timings in 10 times run experiments:\n",
    "        max       min       sub    bit_op    approx      clut       add     total\n",
    "0  0.022528  0.013312  0.015360  0.125952  0.074752  0.072704  0.017408  0.342016\n",
    "1  0.014336  0.011264  0.012288  0.034816  0.025600  0.013312  0.018432  0.130048\n",
    "2  0.016384  0.011264  0.011264  0.027648  0.023552  0.014336  0.014336  0.118784\n",
    "3  0.014336  0.011264  0.012288  0.027648  0.024576  0.013312  0.015360  0.118784\n",
    "4  0.013312  0.011264  0.011264  0.030720  0.025600  0.012288  0.014336  0.118784\n",
    "5  0.013312  0.011264  0.012288  0.026624  0.024576  0.013312  0.014336  0.115712\n",
    "6  0.012288  0.011264  0.011264  0.026624  0.024576  0.013312  0.013312  0.112640\n",
    "7  0.013312  0.011264  0.012288  0.026624  0.024576  0.012288  0.014336  0.114688\n",
    "8  0.013312  0.011264  0.013312  0.027648  0.024576  0.013312  0.014336  0.117760\n",
    "9  0.013312  0.011264  0.012288  0.026624  0.024576  0.012288  0.018432  0.118784\n",
    "\n",
    "GPU: Float32 logsumexp timings in 10 times run experiments:\n",
    "        max       min       sub       exp       sum       log       add     total\n",
    "0  0.212992  0.028672  0.086016  2.698240  0.192512  0.043008  0.013312  3.274752\n",
    "1  0.014336  0.013312  0.013312  0.014336  0.021504  0.015360  0.012288  0.104448\n",
    "2  0.013312  0.012288  0.012288  0.013312  0.020480  0.013312  0.011264  0.096256\n",
    "3  0.013312  0.012288  0.012288  0.013312  0.019456  0.013312  0.011264  0.095232\n",
    "4  0.012288  0.012288  0.012288  0.014336  0.020480  0.013312  0.011264  0.096256\n",
    "5  0.013312  0.011264  0.012288  0.013312  0.020480  0.012288  0.010240  0.093184\n",
    "6  0.012288  0.011264  0.012288  0.013312  0.019456  0.013312  0.012288  0.094208\n",
    "7  0.013312  0.011264  0.012288  0.014336  0.020480  0.012288  0.010240  0.094208\n",
    "8  0.012288  0.012288  0.011264  0.013312  0.020480  0.013312  0.011264  0.094208\n",
    "9  0.012288  0.011264  0.013312  0.012288  0.019456  0.013312  0.011264  0.093184\n",
    "GPU: Four Int32 logsumexp timings in 10 times run experiments:\n",
    "        max       min       sub   bit_com    approx      clut       add     total\n",
    "0  0.023552  0.013312  0.014336  0.126976  0.073728  0.072704  0.018432  0.343040\n",
    "1  0.013312  0.012288  0.013312  0.029696  0.026624  0.012288  0.014336  0.121856\n",
    "2  0.013312  0.011264  0.013312  0.028672  0.025600  0.013312  0.014336  0.119808\n",
    "3  0.013312  0.011264  0.011264  0.027648  0.025600  0.013312  0.014336  0.116736\n",
    "4  0.013312  0.012288  0.012288  0.026624  0.025600  0.013312  0.014336  0.117760\n",
    "5  0.012288  0.011264  0.012288  0.026624  0.026624  0.013312  0.014336  0.116736\n",
    "6  0.013312  0.011264  0.012288  0.027648  0.025600  0.012288  0.013312  0.115712\n",
    "7  0.012288  0.011264  0.012288  0.027648  0.025600  0.013312  0.014336  0.116736\n",
    "8  0.012288  0.011264  0.012288  0.027648  0.024576  0.013312  0.013312  0.114688\n",
    "9  0.012288  0.011264  0.012288  0.026624  0.024576  0.013312  0.014336  0.114688\n",
    "\n",
    "observations:\n",
    "1: our int32 perform better in cpu, gpu for the first time run; see cpu_timings.png and gpu_timings.png\n",
    "2: after first time run, torch logsumexp are much faster than the first time, our functions are not improved much; see above results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36382348",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a25c1ef4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b57abb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
